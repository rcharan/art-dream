{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.applications.vgg19 as vgg19\n",
    "\n",
    "from tensorflow import einsum\n",
    "from tensorflow.keras.layers import Lambda, MaxPool2D, AvgPool2D, Layer, Input, Subtract, Multiply, Add\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utilities')\n",
    "from utilities import load_image, show_image, vgg19_process_image, vgg19_deprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram Matrix Layers\n",
    "def gram_matrix(activations):\n",
    "    result        = tf.linalg.einsum('aijb,aijc->abc', activations, activations)\n",
    "    input_shape   = tf.shape(activations)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require: base_model, style_image, content_image, style_layers, content_layers\n",
    "width = height = 448\n",
    "\n",
    "style_layers    = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "content_layers  = ['block4_conv2']\n",
    "\n",
    "# Build the extractive model\n",
    "base_model.trainable   = False\n",
    "num_style_outputs      = len(style_layers)\n",
    "\n",
    "style_outputs   = []\n",
    "content_outputs = []\n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    if i == 0:\n",
    "        # Custom input size\n",
    "        input_      = tf.keras.layers.Input(shape = (width, height, 3))\n",
    "        current     = input_\n",
    "    elif isinstance(layer, MaxPool2D):\n",
    "        # Replace max pooling with average pooling\n",
    "        pool_config = layer.get_config()\n",
    "        avg_pool    = AvgPool2D().from_config(pool_config)\n",
    "        current     = avg_pool(current)\n",
    "    else:\n",
    "        current = layer(current)\n",
    "\n",
    "    if layer.name in style_layers:\n",
    "        # Compute gram matrix as a style output\n",
    "        style_outputs.append(Lambda(gram_matrix, name = f'gram_{layer.name}')(current))\n",
    "    if layer.name in content_layers:\n",
    "        # Keep some convolutional layers as outputs\n",
    "        content_outputs.append(current)\n",
    "\n",
    "extract_model = Model(inputs = [input_], outputs = style_outputs + content_outputs, name = 'extractive_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source layers (with no/fake inputs)\n",
    "class Source(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        self.kernel = self.add_weight(name='kernel', shape=self.output_dim, initializer='uniform', trainable=True)\n",
    "        super().build(input_shapes)  \n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.kernel\n",
    "\n",
    "    def compute_output_shape(self):\n",
    "        return self.output_dim\n",
    "    \n",
    "    def get_params(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'output_dim' : self.output_dim}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fake_input = Input(())\n",
    "source     = Source((1 ,width, height, 3))\n",
    "\n",
    "extract_outputs  = extract_model(source(fake_input))\n",
    "\n",
    "style_content_weighting = 0.01\n",
    "\n",
    "style_layer_weighting   = tf.constant(1/len(style_outputs) * style_content_weighting * (1/4))\n",
    "content_layer_weighting = tf.constant((1/2))\n",
    "\n",
    "transfer_inputs  = [fake_input]\n",
    "transfer_outputs = []\n",
    "for i, output in enumerate(extract_outputs):\n",
    "    batch_input_shape = output.shape\n",
    "    input_     = Input(shape = batch_input_shape[1:], batch_size=batch_input_shape[0])\n",
    "    transfer_inputs.append(input_)\n",
    "    difference = Subtract()([input_, output])\n",
    "    square     = Lambda(tf.square, name = f'square_{i}')(difference)\n",
    "    # Determine if the output is style or content\n",
    "    if i < num_style_outputs:\n",
    "        reduce = Lambda(lambda t : tf.reduce_mean(t, axis = [1,2]), name = f'mean_{i}')(square)\n",
    "        scale  = Lambda(lambda x : x * style_layer_weighting, name = f'weight_{i}')(reduce)\n",
    "    else:\n",
    "        reduce = Lambda(lambda t : tf.reduce_sum(t, axis = [1,2,3]), name = f'sum_{i}')(square)\n",
    "        scale  = Lambda(lambda x : x * content_layer_weighting, name = f'weight_{i}')(reduce)\n",
    "    transfer_outputs.append(scale)\n",
    "output = Add()(transfer_outputs)\n",
    "\n",
    "transfer_model = Model(inputs = transfer_inputs, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load test cases\n",
    "content_path = '../dream-base-images/marco3.png'\n",
    "style_path   = '../dataset/images/train/Max Ernst/21433.jpg'\n",
    "\n",
    "content_image = load_image(content_path, cast = tf.float32)\n",
    "style_image = load_image(style_path, cast = tf.uint8)\n",
    "\n",
    "# Pre-process the style and content images\n",
    "images = [style_image, content_image]\n",
    "for i, image in enumerate(images):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [width, height])\n",
    "    image = vgg19_process_image(image)\n",
    "    image = tf.expand_dims(image, axis = 0)\n",
    "    images[i] = image\n",
    "\n",
    "style_image, content_image = images\n",
    "\n",
    "# Compute the target activations\n",
    "style_activations   = extract_model(style_image  )[:num_style_outputs]\n",
    "content_activations = extract_model(content_image)[num_style_outputs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = tf.constant(0, shape = (1,), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_input = tuple([dummy] + style_activations + content_activations)\n",
    "transfer_input = tf.data.Dataset.from_tensors((transfer_input, dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precomputed_loss(dummy, loss):\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.compile(optimizer = 'adam', loss = precomputed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transfer_model.fit(transfer_input, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(transfer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
