{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Dense, MaxPool2D, MaxPooling2D, AvgPool2D, Flatten, Layer, Dropout, Input, Subtract, Multiply, Add, InputLayer\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils  import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utilities')\n",
    "from utilities import (\n",
    "    load_image,\n",
    "    show_image,\n",
    "    vgg19_process_image,\n",
    "    vgg19_deprocess_image,\n",
    "    precomputed_loss,\n",
    "    get_image_from_model,\n",
    "    gram_matrix, \n",
    "    Source,\n",
    "    Timer,\n",
    "    class_names,\n",
    "    dummy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Style Extraction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model('../classification/logs/models/vgg19-INet-down2-b.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = height = 896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "input_ = Input(shape = (width, height, 3))\n",
    "signal = input_\n",
    "\n",
    "style_outputs = []\n",
    "for layer in base_model.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    elif isinstance(layer, Dense):\n",
    "        break\n",
    "    else:\n",
    "        signal = layer(signal)\n",
    "    \n",
    "    if layer.name in style_layers:\n",
    "        style_outputs.append(Lambda(gram_matrix, name = f'gram_{layer.name}')(signal))\n",
    "        \n",
    "extract_model = Model(inputs = [input_], outputs = style_outputs, name = 'extractive_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Some Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.305 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Pick an artwork for each artist, compute activations\n",
    "artists = class_names.tolist()\n",
    "\n",
    "Timer.start()\n",
    "np.random.seed(42)\n",
    "def get_activations(artist):\n",
    "    artist_dir = f'../dataset/images/train/{artist}/'\n",
    "    possible_choices = os.listdir(artist_dir)\n",
    "    possible_choices = filter(lambda f : f.endswith('.jpg'), possible_choices)\n",
    "    choice = np.random.choice(list(possible_choices))\n",
    "    image  = load_image(artist_dir + choice)\n",
    "    image  = vgg19_process_image(image)\n",
    "    image  = tf.image.resize(image, (width, height))\n",
    "    image  = tf.expand_dims(image, axis = 0)\n",
    "    activations = extract_model(image)\n",
    "    return activations\n",
    "    \n",
    "activations = [get_activations(artist) for artist in artists]\n",
    "Timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Save those activations to disk\n",
    "Timer.start()\n",
    "for artist, activations in zip(artists, activations):\n",
    "    file_name = f'./style-activations/{artist}.npz'\n",
    "    np_data   = [tensor.numpy() for tensor in activations]\n",
    "    np.savez(file_name, *np_data)\n",
    "Timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Dreaming Model\n",
    "Inputs will be: a dummy input and  desired style activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be moved to utilities\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "\n",
    "class RemainImage(Constraint):\n",
    "    def __init__(self, rate = 1.0):\n",
    "        super().__init__()\n",
    "        self.rate = rate\n",
    "    \n",
    "    def __call__(self, kernel):\n",
    "        return (self.rate       * tf.clip_by_value(kernel, -150, 150) + \n",
    "                (1 - self.rate) * kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_content_weighting = 0.01\n",
    "\n",
    "style_layer_weighting   = tf.constant(1/len(style_outputs) * style_content_weighting * (1/4))\n",
    "\n",
    "input_       = Input(shape = (), batch_size = 1)\n",
    "image_layer  = Source((1, width, height, 3), name = 'image', kernel_constraint = RemainImage(0.9))\n",
    "signal       = image_layer(input_)\n",
    "\n",
    "output_layers = {\n",
    "    'dense_2'            : 1.0,\n",
    "    'block4_conv1'       : 0.1,\n",
    "    'block5_conv1'       : 0.1,\n",
    "}\n",
    "max_pool_extracted_layers = False\n",
    "\n",
    "# Alias\n",
    "if 'dense_2' in output_layers:\n",
    "    output_layers['fc2'] = output_layers['dense_2']\n",
    "if 'dense_1' in output_layers:\n",
    "    output_layers['fc1'] = output_layers['dense_1']\n",
    "\n",
    "output_count = 0\n",
    "outputs = []\n",
    "inputs  = [input_]\n",
    "def get_summary_function(weight):\n",
    "    def summary_fn(activation):\n",
    "        # activation = tf.square(activation)\n",
    "        axes       = tf.range(1, tf.rank(activation))\n",
    "        activation = tf.reduce_mean(activation, axis = axes)\n",
    "        return activation * (-1) * weight\n",
    "    return summary_fn\n",
    "base_model_layers = base_model.layers\n",
    "\n",
    "# Add resampling before flattening to allow larger images\n",
    "flatten_index     = next(filter(\n",
    "                            lambda i : isinstance(base_model_layers[i], Flatten), \n",
    "                            range(len(base_model_layers))))\n",
    "resize_layer      = Lambda(lambda tensor : tf.image.resize(tensor, (7,7), method = 'gaussian'),\n",
    "                           name = 'resize')\n",
    "base_model_layers.insert(flatten_index, resize_layer)\n",
    "\n",
    "for layer in base_model_layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    elif isinstance(layer, Dropout):\n",
    "        continue\n",
    "    elif isinstance(layer, MaxPooling2D):\n",
    "        layer = AvgPool2D().from_config(layer.get_config())\n",
    "    elif layer.name in ['dense_2', 'fc2']:\n",
    "        # Top layer requires different activations\n",
    "        top_config = layer.get_config()\n",
    "        top_config['activation'] = 'elu'\n",
    "        top_layer = layer\n",
    "        layer     = layer.from_config(top_config)\n",
    "\n",
    "    signal = layer(signal)\n",
    "    \n",
    "    if layer.name in output_layers:\n",
    "        output_count += 1\n",
    "        summarizer_layer = Lambda(get_summary_function(output_layers[layer.name]), name = f'compute_gain_{output_count}')\n",
    "        # Optional: max pool the layer first (!!)\n",
    "        if max_pool_extracted_layers:\n",
    "            max_pool = GlobalMaxPooling2D(name = f'focus_{output_count}')\n",
    "            total_activation = summarizer_layer(max_pool(signal))\n",
    "        else:\n",
    "            total_activation = summarizer_layer(signal)\n",
    "        outputs.append(total_activation)\n",
    "\n",
    "    if layer.name in style_layers:\n",
    "        output_count += 1\n",
    "        gram_matrix = Lambda(gram_matrix, name = f'gram_{layer.name}')\n",
    "        gram_signal = gram_matrix(signal)\n",
    "        batch_input_shape = gram_signal.shape\n",
    "        input_     = Input(shape = batch_input_shape[1:], batch_size=batch_input_shape[0])\n",
    "        inputs.append(input_)\n",
    "        difference = Subtract()([input_, gram_signal])\n",
    "        square     = Lambda(tf.square, name = f'square_{output_count}')(difference)\n",
    "        reduce = Lambda(lambda t : tf.reduce_mean(t, axis = [1,2]), name = f'mean_{output_count}')(square)\n",
    "        scale  = Lambda(lambda x : x * style_layer_weighting, name = f'weight_{output_count}')(reduce)\n",
    "        outputs.append(scale)\n",
    "\n",
    "final_output = Add()(outputs)\n",
    "        \n",
    "model = Model(inputs = inputs, outputs = final_output)\n",
    "\n",
    "# Weights for the top layer (with artist activations)\n",
    "#  were reset; restore them\n",
    "top_layer.set_weights(base_model.layers[-1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = '../dream-base-images/IMG_0325.jpg'\n",
    "image = load_image(image_path, cast = tf.float32)\n",
    "image = tf.image.resize(image, [width, height])\n",
    "image = vgg19_process_image(image)\n",
    "image = tf.expand_dims(image, axis = 0)\n",
    "\n",
    "image_layer = model.get_layer('image')\n",
    "image_layer.set_weights([image.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_path   = '../dataset/images/train/Max Ernst/21433.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = tf.data.Dataset.from_tensors((dummy, dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_style_and_content(style_image, content_image, extract_model, num_style_outputs):\n",
    "    # Pre-process the style and content images\n",
    "    images = [style_image, content_image]\n",
    "    for i, image in enumerate(images):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = tf.image.resize(image, [width, height])\n",
    "        image = vgg19_process_image(image)\n",
    "        image = tf.expand_dims(image, axis = 0)\n",
    "        images[i] = image\n",
    "\n",
    "    style_image, content_image = images\n",
    "\n",
    "    # Compute the target activations\n",
    "    style_activations   = extract_model(style_image  )[:num_style_outputs]\n",
    "    content_activations = extract_model(content_image)[num_style_outputs:]\n",
    "    \n",
    "    # Prepare a dataset that can be passed to the extract model\n",
    "    dummy = tf.constant(0, shape = (1,), dtype = tf.float32)\n",
    "    transfer_input = tuple([dummy] + style_activations + content_activations)\n",
    "    transfer_input = tf.data.Dataset.from_tensors((transfer_input, dummy))\n",
    "    \n",
    "    return transfer_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Poor man's learning rate scheduler; for tuning\n",
    "fast_adam = tf.optimizers.Adam(learning_rate = 10.0)\n",
    "transfer_model.compile(optimizer = fast_adam, loss = precomputed_loss)\n",
    "history = transfer_model.fit(style_and_content_data.repeat(100), epochs = 3)\n",
    "\n",
    "fast_adam = tf.optimizers.Adam(learning_rate = 1.0)\n",
    "transfer_model.compile(optimizer = fast_adam, loss = precomputed_loss)\n",
    "history = transfer_model.fit(style_and_content_data.repeat(100), epochs = 4)\n",
    "\n",
    "fast_adam = tf.optimizers.Adam(learning_rate = 0.5)\n",
    "transfer_model.compile(optimizer = fast_adam, loss = precomputed_loss)\n",
    "history = transfer_model.fit(style_and_content_data.repeat(100), epochs = 5)\n",
    "\n",
    "fast_adam = tf.optimizers.Adam(learning_rate = 0.1)\n",
    "transfer_model.compile(optimizer = fast_adam, loss = precomputed_loss)\n",
    "history = transfer_model.fit(style_and_content_data.repeat(100), epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_image_from_model(transfer_model)\n",
    "show_image(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
